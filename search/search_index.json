{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to fdaPDE functional data analysis with partial differential equation regularization. Pagina home di prova...","title":"Home"},{"location":"#welcome-to-fdapde","text":"functional data analysis with partial differential equation regularization. Pagina home di prova...","title":"Welcome to fdaPDE"},{"location":"BFGSOptimizer/","text":"","title":"BFGS"},{"location":"DifferentiableScalarField/","text":"DifferentiableScalarField core/OPT/ScalarField.h Extends: ScalarField Template class used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose gradient function \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N is known analitically at every point. template < unsigned int N > class DifferentiableScalarField : public ScalarField < N > { ... }; Methods DifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression std :: function < SVector < N > ( SVector < N > ) > derive () const override ; Returns the analytical expression of the field's gradient \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N as a Callable object. Examples Define a scalar field f(x,y) = 2x^2 - 2y^2x having exact gradient equal to \\nabla f = [4x - 2y^2, -4yx]^T 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ] - 2 * std :: pow ( x [ 1 ], 2 ), -4 * x [ 1 ] * x [ 0 ]}); }; // define differentiable field DifferentiableScalarField < 2 > field ( g , dg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVector < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of gradient at point SVector < 2 > grad = field . getGradientApprox ( SVector < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; // evaluate exact gradient at point SVector < 2 > exactGrad = field . derive ()( SVector < 2 > ({ 2 , 1 })); std :: cout << \"exact gradient at point\" << std :: endl ; std :: cout << exactGrad << std :: endl ;","title":"DifferentiableScalarField"},{"location":"DifferentiableScalarField/#differentiablescalarfield","text":"core/OPT/ScalarField.h Extends: ScalarField Template class used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose gradient function \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N is known analitically at every point. template < unsigned int N > class DifferentiableScalarField : public ScalarField < N > { ... };","title":"DifferentiableScalarField"},{"location":"DifferentiableScalarField/#methods","text":"DifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression std :: function < SVector < N > ( SVector < N > ) > derive () const override ; Returns the analytical expression of the field's gradient \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N as a Callable object.","title":"Methods"},{"location":"DifferentiableScalarField/#examples","text":"Define a scalar field f(x,y) = 2x^2 - 2y^2x having exact gradient equal to \\nabla f = [4x - 2y^2, -4yx]^T 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ] - 2 * std :: pow ( x [ 1 ], 2 ), -4 * x [ 1 ] * x [ 0 ]}); }; // define differentiable field DifferentiableScalarField < 2 > field ( g , dg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVector < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of gradient at point SVector < 2 > grad = field . getGradientApprox ( SVector < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; // evaluate exact gradient at point SVector < 2 > exactGrad = field . derive ()( SVector < 2 > ({ 2 , 1 })); std :: cout << \"exact gradient at point\" << std :: endl ; std :: cout << exactGrad << std :: endl ;","title":"Examples"},{"location":"ExactNewtonOptimizer/","text":"ExactNewtonOptimizer Extends NewtonOptimizer Template class to optimize a given TwiceDifferentiableScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class ExactNewtonOptimizer : public NewtonOptimizer < N > { ... }; Note The implementation of Newton's method provided by this class relies on the exact analytical expression of gradient and hessian of the field. See parent class NewtonOptimizer in case you want to use numerically approximations for these quantities or you have no analytical expressions for them. Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting . Methods ExactNewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimumExact ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , const TwiceDifferentiableScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. TwiceDifferentiableScalarField<N>& objective The objective function to optimize encoded as a TwiceDifferentiableScalarField to have access to exact gradient and hessian expressions. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations. Examples The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"ExactNewtonOptimizer"},{"location":"ExactNewtonOptimizer/#exactnewtonoptimizer","text":"Extends NewtonOptimizer Template class to optimize a given TwiceDifferentiableScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class ExactNewtonOptimizer : public NewtonOptimizer < N > { ... }; Note The implementation of Newton's method provided by this class relies on the exact analytical expression of gradient and hessian of the field. See parent class NewtonOptimizer in case you want to use numerically approximations for these quantities or you have no analytical expressions for them. Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting .","title":"ExactNewtonOptimizer"},{"location":"ExactNewtonOptimizer/#methods","text":"ExactNewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimumExact ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , const TwiceDifferentiableScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. TwiceDifferentiableScalarField<N>& objective The objective function to optimize encoded as a TwiceDifferentiableScalarField to have access to exact gradient and hessian expressions. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations.","title":"Methods"},{"location":"ExactNewtonOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"Examples"},{"location":"Function/","text":"Function","title":"Function"},{"location":"Function/#function","text":"","title":"Function"},{"location":"GradientDescentOptimizer/","text":"","title":"GradientDescentOptimizer"},{"location":"GridOptimizer/","text":"GridOptimizer core/OPT/GridOptimizer.h Extends: Optimizer Template class to optimize a given ScalarField over an N-dimensional grid of equidistant nodes. template < unsigned int N > class GridOptimizer { ... }; The class simulates the construction of an N-dimensional rectangle [a_1, b_1] \\times \\ldots \\times [a_N, b_N] splitted along each interval according to a given grid step (the grid step can possibly differ for each dimension). The search is performed using an exhaustive search over the N-dimensional grid. Warning Since this optimization algorithm applies just an exhaustive search without any heuristic the resulting time complexity is exponential in the dimension of the grid N. The method can be very slow for values of N bigger than 2. Methods GridOptimizer ( std :: array < std :: pair < double , double > , N > domain , std :: array < double , N > steps , const ScalarField < N >& objective_ ) Constructor. It sets the shape of the domain where to search for the optimum of the objective as well as the grid step for each dimension. Args Description std::array<std::pair<double,double>,N> An array of exactly N pairs where pair i represents infimum and superior limit of the interval [a_i, b_i] along the i -th dimension std::array<double,N> steps An array of exactly N doubles where the i -th element defines the grid step along the i -th dimension const ScalarField<N>& objective_ The objective function to optimize std :: pair < SVector < N > , double > findMinimum () override ; Apply the search strategy to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in the grid where the minimum is reached while the second one is the actual minimum value reached by the objective. Info During the search no grid is actually stored in memory making the call at least memory efficient. Examples The following code finds the minimum of function g(x,y) = 2x^2 - 2y^2 over a grid of points defined in [1,5] \\times [1,5] \\subset \\mathbb{R}^2 with grid step 0.01 for each dimension 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // define target to optimize: 2*x^2 - 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ); }; // create a scalar field ScalarField < 2 > objective ( g ); // perform a 2D grid optimization // set optimization domain: [1,5] x [1,5] std :: array < std :: pair < double , double > , 2 > domain2D = { std :: pair < double , double > ( 1 , 5 ), std :: pair < double , double > ( 1 , 5 ) }; // set grid step size std :: array < double , 2 > step2D = { 0.01 , 0.01 }; // create optimizer GridOptimizer < 2 > opt2D ( domain2D , lambda2D , objective ); // find minimum of g std :: pair < array < double , 2 > , double > min_g = opt2D . findMinimum ();","title":"GridOptimizer"},{"location":"GridOptimizer/#gridoptimizer","text":"core/OPT/GridOptimizer.h Extends: Optimizer Template class to optimize a given ScalarField over an N-dimensional grid of equidistant nodes. template < unsigned int N > class GridOptimizer { ... }; The class simulates the construction of an N-dimensional rectangle [a_1, b_1] \\times \\ldots \\times [a_N, b_N] splitted along each interval according to a given grid step (the grid step can possibly differ for each dimension). The search is performed using an exhaustive search over the N-dimensional grid. Warning Since this optimization algorithm applies just an exhaustive search without any heuristic the resulting time complexity is exponential in the dimension of the grid N. The method can be very slow for values of N bigger than 2.","title":"GridOptimizer"},{"location":"GridOptimizer/#methods","text":"GridOptimizer ( std :: array < std :: pair < double , double > , N > domain , std :: array < double , N > steps , const ScalarField < N >& objective_ ) Constructor. It sets the shape of the domain where to search for the optimum of the objective as well as the grid step for each dimension. Args Description std::array<std::pair<double,double>,N> An array of exactly N pairs where pair i represents infimum and superior limit of the interval [a_i, b_i] along the i -th dimension std::array<double,N> steps An array of exactly N doubles where the i -th element defines the grid step along the i -th dimension const ScalarField<N>& objective_ The objective function to optimize std :: pair < SVector < N > , double > findMinimum () override ; Apply the search strategy to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in the grid where the minimum is reached while the second one is the actual minimum value reached by the objective. Info During the search no grid is actually stored in memory making the call at least memory efficient.","title":"Methods"},{"location":"GridOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 - 2y^2 over a grid of points defined in [1,5] \\times [1,5] \\subset \\mathbb{R}^2 with grid step 0.01 for each dimension 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 // define target to optimize: 2*x^2 - 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ); }; // create a scalar field ScalarField < 2 > objective ( g ); // perform a 2D grid optimization // set optimization domain: [1,5] x [1,5] std :: array < std :: pair < double , double > , 2 > domain2D = { std :: pair < double , double > ( 1 , 5 ), std :: pair < double , double > ( 1 , 5 ) }; // set grid step size std :: array < double , 2 > step2D = { 0.01 , 0.01 }; // create optimizer GridOptimizer < 2 > opt2D ( domain2D , lambda2D , objective ); // find minimum of g std :: pair < array < double , 2 > , double > min_g = opt2D . findMinimum ();","title":"Examples"},{"location":"NewtonOptimizer/","text":"NewtonOptimizer core/OPT/NewtonOptimizer.h Extends: IterativeOptimizer Template class to optimize a given ScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class NewtonOptimizer { ... }; Note The implementation of Newton's method provided by this class works by numerically approximating both gradient and hessian of the field at the given point. See ExactNewtonOptimizer in case you want to use the analytical expression of these quantites during the iterative process Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting . Methods NewtonOptimizer ( double step_ , const SVector < N >& x0_ , unsigned int maxIteration_ , double tolerance_ , const ScalarField < N >& objective_ ) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. ScalarField<N>& objective The objective function to optimize. std :: pair < SVector < N > , double > findMinimum () override ; Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Info Default stopping condition: the method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations. Tip You can control the optimizer by exploiting the IterativeOptimizer interface. Examples The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization double lambda = 0.01 ; // learning rate unsigned int max_iterations = 1000 ; // max number of iterations double tolerance = 0.001 ; // tolerance // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda , SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective ); // find minimum of objective std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ();","title":"NewtonOptimizer"},{"location":"NewtonOptimizer/#newtonoptimizer","text":"core/OPT/NewtonOptimizer.h Extends: IterativeOptimizer Template class to optimize a given ScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class NewtonOptimizer { ... }; Note The implementation of Newton's method provided by this class works by numerically approximating both gradient and hessian of the field at the given point. See ExactNewtonOptimizer in case you want to use the analytical expression of these quantites during the iterative process Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting .","title":"NewtonOptimizer"},{"location":"NewtonOptimizer/#methods","text":"NewtonOptimizer ( double step_ , const SVector < N >& x0_ , unsigned int maxIteration_ , double tolerance_ , const ScalarField < N >& objective_ ) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. ScalarField<N>& objective The objective function to optimize. std :: pair < SVector < N > , double > findMinimum () override ; Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Info Default stopping condition: the method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations. Tip You can control the optimizer by exploiting the IterativeOptimizer interface.","title":"Methods"},{"location":"NewtonOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization double lambda = 0.01 ; // learning rate unsigned int max_iterations = 1000 ; // max number of iterations double tolerance = 0.001 ; // tolerance // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda , SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective ); // find minimum of objective std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ();","title":"Examples"},{"location":"Optimizer/","text":"The optimization module provides a set of general routines for optimizing a generic ScalarField f : \\mathbb{R}^N \\rightarrow \\mathbb{R} . Optimizer core/OPT/Optimizer.h Extends: - Base interface for the whole optimization module. template < unsigned int N > class Optimizer { ... }; Info The template parameter N denotes the dimension of the space where to search for the optimum point. This information should be known at compile time in order to define the data structure required during the optimization. Developer's advice Any class meant to work as an optimizer must derive this class or one of its direct child classes. Methods offered by the interface virtual std :: pair < SVector < N > , double > findMinimum (); The main entry point for the optimization routine. A call to this method should produce as output a std::pair<SVector<N>, double> where the first component is the point where the minimum of the objective is reached while the second component is the actual minimum value found. The algorithm used for producing the result is implementation dependent. IterativeOptimizer core/OPT/IterativeOptimizer.h Extends: Optimizer Template abstract class representing a general iterative optimizer. An iterative optimizer is any optimization routine which can be schematized as follow: 1 2 3 4 5 6 7 8 9 let x [ 0 ] the starting point let k = 0 while ( some stopping condition is not met ){ // update the current solution x [ k + 1 ] = x [ k ] + a [ k ] * d [ k ]; k ++ ; } return x [ k ]; Even if algorithms can drammatically differ in the computation of the update step this general footprint is still mantained. For this reason a rich family of optimization algorithms fall under this class. The IterativeOptimizer interface should be used to inform the user that an iterative optimization procedure is implemented under the hood. template < unsigned int N > class IterativeOptimizer : public Optimizer < N > { ... }; Info The IterativeOptimizer class does not implement the general schema of an iterative optimizer, whose implementation is left to the derived classes. Forcing any deriving class to follow a too rigid schema could introduce useless complications in the implementation of the optimization procedure. This class introduce anyway the possibility to control in some way the flow of execution of an iterative optimizer. For example it would be possible to monitor during the execution of the algorithm itself some quantities of interest for the particular problem at hand and force the stop of the procedure on the base of some custom stopping criterion bypassing the default one. The way this mechanism is reached is by extending any concrete implementation of the IterativeOptimizer class and overloading the wanted methods exposed by IterativeOptimizer itself. Developer's advice Is up to the derived classes of IterativeOptimizer to implement properly the stated mechanism. When you want to give the possibility to execute a (possibly) custom action of a (possibly) deriving class you should insert i.e. this->init() for executing a custom initialization. See the semantic of IterativeOptimizer ' methods to see what kind of customizations can be added and decide the proper place where to execute the action. Methods virtual void init (); This method should be called once before entering the iterative loop. virtual void preStep (); This method should be inserted in the iterative loop, possibly as first loop instruction. In any case insert it before the update step. virtual void postStep (); This method should be inserted in the iterative loop. Insert it after the update step. A good place is immediately after the error update. virtual bool stopCondition (); This method should be called in a while of for statement togheter with the default termination criteria. The good practice is to write something like while ( ! default_stopping_condition () && ! this -> stopCondition ()) { ... } It must return false if the custom stopping condition is not met. virtual void finalize (); This method should be called once outside the iterative loop, possibly right before the return statement. As an example consider the implementation of the optimization routine for the GradientDescent 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // gradient descent optimization routine template < unsigned int N > std :: pair < SVector < N > , double > GradientDescentOptimizer < N >:: findMinimum (){ this -> init (); // execute custom action // algorithm initialization x_old = x0 ; unsigned int numIteration = 0 ; // standard termination criteria based on l^2 norm of the gradient error = objective . derive ()( x_old ). squaredNorm (); while ( numIteration < maxIteration && error > tolerance && ! this -> stopCondition ()){ this -> preStep (); // execute custom action // compute exact gradient gradientExact = objective . derive ()( x_old ); // update step x_new = x_old - step * gradientExact ; // error update: standard termination criteria based on l^2 norm of the gradient error = gradientExact . squaredNorm (); this -> postStep (); // execute custom action // prepare next iteration x_old = x_new ; numIteration ++ ; } this -> finalize (); // execute custom action return std :: pair < SVector < N > , double > ( x_old , objective ( x_old )); } Tip The IterativeOptimizer class offers a std::unordered_map<std::string, std::list<double>> controllerData which can be used by a customization to store or record values needed to perform custom actions. Use the init() method to initialize any field you might require before the actual optimization starts.","title":"Optimizer"},{"location":"Optimizer/#optimizer","text":"core/OPT/Optimizer.h Extends: - Base interface for the whole optimization module. template < unsigned int N > class Optimizer { ... }; Info The template parameter N denotes the dimension of the space where to search for the optimum point. This information should be known at compile time in order to define the data structure required during the optimization. Developer's advice Any class meant to work as an optimizer must derive this class or one of its direct child classes.","title":"Optimizer"},{"location":"Optimizer/#methods-offered-by-the-interface","text":"virtual std :: pair < SVector < N > , double > findMinimum (); The main entry point for the optimization routine. A call to this method should produce as output a std::pair<SVector<N>, double> where the first component is the point where the minimum of the objective is reached while the second component is the actual minimum value found. The algorithm used for producing the result is implementation dependent.","title":"Methods offered by the interface"},{"location":"Optimizer/#iterativeoptimizer","text":"core/OPT/IterativeOptimizer.h Extends: Optimizer Template abstract class representing a general iterative optimizer. An iterative optimizer is any optimization routine which can be schematized as follow: 1 2 3 4 5 6 7 8 9 let x [ 0 ] the starting point let k = 0 while ( some stopping condition is not met ){ // update the current solution x [ k + 1 ] = x [ k ] + a [ k ] * d [ k ]; k ++ ; } return x [ k ]; Even if algorithms can drammatically differ in the computation of the update step this general footprint is still mantained. For this reason a rich family of optimization algorithms fall under this class. The IterativeOptimizer interface should be used to inform the user that an iterative optimization procedure is implemented under the hood. template < unsigned int N > class IterativeOptimizer : public Optimizer < N > { ... }; Info The IterativeOptimizer class does not implement the general schema of an iterative optimizer, whose implementation is left to the derived classes. Forcing any deriving class to follow a too rigid schema could introduce useless complications in the implementation of the optimization procedure. This class introduce anyway the possibility to control in some way the flow of execution of an iterative optimizer. For example it would be possible to monitor during the execution of the algorithm itself some quantities of interest for the particular problem at hand and force the stop of the procedure on the base of some custom stopping criterion bypassing the default one. The way this mechanism is reached is by extending any concrete implementation of the IterativeOptimizer class and overloading the wanted methods exposed by IterativeOptimizer itself. Developer's advice Is up to the derived classes of IterativeOptimizer to implement properly the stated mechanism. When you want to give the possibility to execute a (possibly) custom action of a (possibly) deriving class you should insert i.e. this->init() for executing a custom initialization. See the semantic of IterativeOptimizer ' methods to see what kind of customizations can be added and decide the proper place where to execute the action.","title":"IterativeOptimizer "},{"location":"Optimizer/#methods","text":"virtual void init (); This method should be called once before entering the iterative loop. virtual void preStep (); This method should be inserted in the iterative loop, possibly as first loop instruction. In any case insert it before the update step. virtual void postStep (); This method should be inserted in the iterative loop. Insert it after the update step. A good place is immediately after the error update. virtual bool stopCondition (); This method should be called in a while of for statement togheter with the default termination criteria. The good practice is to write something like while ( ! default_stopping_condition () && ! this -> stopCondition ()) { ... } It must return false if the custom stopping condition is not met. virtual void finalize (); This method should be called once outside the iterative loop, possibly right before the return statement. As an example consider the implementation of the optimization routine for the GradientDescent 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // gradient descent optimization routine template < unsigned int N > std :: pair < SVector < N > , double > GradientDescentOptimizer < N >:: findMinimum (){ this -> init (); // execute custom action // algorithm initialization x_old = x0 ; unsigned int numIteration = 0 ; // standard termination criteria based on l^2 norm of the gradient error = objective . derive ()( x_old ). squaredNorm (); while ( numIteration < maxIteration && error > tolerance && ! this -> stopCondition ()){ this -> preStep (); // execute custom action // compute exact gradient gradientExact = objective . derive ()( x_old ); // update step x_new = x_old - step * gradientExact ; // error update: standard termination criteria based on l^2 norm of the gradient error = gradientExact . squaredNorm (); this -> postStep (); // execute custom action // prepare next iteration x_old = x_new ; numIteration ++ ; } this -> finalize (); // execute custom action return std :: pair < SVector < N > , double > ( x_old , objective ( x_old )); } Tip The IterativeOptimizer class offers a std::unordered_map<std::string, std::list<double>> controllerData which can be used by a customization to store or record values needed to perform custom actions. Use the init() method to initialize any field you might require before the actual optimization starts.","title":"Methods"},{"location":"R/","text":"RfdaPDE This class is the main entry point to the core C++ library from R. Note fdaPDE uses Rcpp and RcppEigen to interface the core library to R. For more informations see Rcpp: Seamless R and C++ Integration RcppEigen: R and Eigen via Rcpp The fdaPDE interface can be loaded from R using the following instructions library(\"Rcpp\") Rcpp::sourceCpp(\"RfdaPDE.cpp\") # create an fdaPDE interface accessible from R fdaPDE_interface = new(RfdaPDE) # you can now access methods using the $ notation fdaPDE_interface$qualcosa ... Interface extension To provide new functionalities to the outside of the C++ core library is required to add a method to the RfdaPDE class working as entry point.","title":"R Interface"},{"location":"R/#rfdapde","text":"This class is the main entry point to the core C++ library from R. Note fdaPDE uses Rcpp and RcppEigen to interface the core library to R. For more informations see Rcpp: Seamless R and C++ Integration RcppEigen: R and Eigen via Rcpp The fdaPDE interface can be loaded from R using the following instructions library(\"Rcpp\") Rcpp::sourceCpp(\"RfdaPDE.cpp\") # create an fdaPDE interface accessible from R fdaPDE_interface = new(RfdaPDE) # you can now access methods using the $ notation fdaPDE_interface$qualcosa ...","title":"RfdaPDE"},{"location":"R/#interface-extension","text":"To provide new functionalities to the outside of the C++ core library is required to add a method to the RfdaPDE class working as entry point.","title":"Interface extension"},{"location":"ScalarField/","text":"ScalarField core/OPT/ScalarField.h Extends: - A template class for handling scalar fields f : \\mathbb{R}^N \\rightarrow \\mathbb{R} template < unsigned int N > class ScalarField { ... }; Note A field wrapped by this template doesn't guarantee any regularity condition. You can wrap any scalar field which is just evaluable at any point. The definition of a formal derivative is not required. See DifferentiableScalarField or TwiceDifferentiableScalarField for inherited classes which assume specific regularity conditions. Methods ScalarField ( std :: function < double ( SVector < N > ) > f_ ) Just sets the internal representation of the field to the functional f_ passed as argument. Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output inline double evaluateAtPoint ( const Point < N >& x ) Returns the evaluation of the field at point x. Args Description const SVector<N>& x The point in \\mathbb{R}^N where to evaluate the field inline double operator ()( SVector < N >& x ) Returns the evaluation of the field at point x. Preserves the std::function syntax. Args Description SVector<N>& x The point in \\mathbb{R}^N where to evaluate the field SVector < N > getGradientApprox ( const SVector < N >& x , double step ) const Returns the numerical approximation of the gradient of f at point x. The partial derivatives of the field are approximated via central differences according to the following expression ( e_i denoting the normal unit vector along direction i ) \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x - he_i)}{2h} Args Description const SVector<N>& x The point in \\mathbb{R}^N where to approximate the gradient of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. SMatrix < N > getHessianApprox ( const SVector < N >& x , double step ) const Returns the numerical approximation of the hessian matrix of f at point x. The partial derivatives of the field are approximated via central differences according to the following expressions ( e_i denoting the normal unit vector along direction i ) \\begin{aligned} &\\frac{\\partial^2 f}{\\partial x_i \\partial x_i} &&\\approx \\frac{-f(x + 2he_i) + 16f(x + he_i) - 30f(x) + 16f(x - he_i) - f(x - 2he_i)}{12h^2} \\\\ &\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} &&\\approx \\frac{f(x + he_i + he_j) - f(x + he_i - he_j) - 16f(x - he_i + he_j) + f(x - he_i - he_j)}{4h^2} \\end{aligned} Args Description const SVector<N>& x The point in \\mathbb{R}^N where to approximate the hessian of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // create ScalarField ScalarField < 2 > field ( g ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << fun . evaluateAtPoint ( SVector < 2 > ( 4 , 1 )) << std :: endl ; SVector < 2 > grad = fun . getGradientApprox ( SVector < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; SMatrix < 2 > hessian = fun . getHessianApprox ( SVector < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of hessian at point\" << std :: endl ; std :: cout << hessian << std :: endl ;","title":"ScalarField"},{"location":"ScalarField/#scalarfield","text":"core/OPT/ScalarField.h Extends: - A template class for handling scalar fields f : \\mathbb{R}^N \\rightarrow \\mathbb{R} template < unsigned int N > class ScalarField { ... }; Note A field wrapped by this template doesn't guarantee any regularity condition. You can wrap any scalar field which is just evaluable at any point. The definition of a formal derivative is not required. See DifferentiableScalarField or TwiceDifferentiableScalarField for inherited classes which assume specific regularity conditions.","title":"ScalarField"},{"location":"ScalarField/#methods","text":"ScalarField ( std :: function < double ( SVector < N > ) > f_ ) Just sets the internal representation of the field to the functional f_ passed as argument. Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output inline double evaluateAtPoint ( const Point < N >& x ) Returns the evaluation of the field at point x. Args Description const SVector<N>& x The point in \\mathbb{R}^N where to evaluate the field inline double operator ()( SVector < N >& x ) Returns the evaluation of the field at point x. Preserves the std::function syntax. Args Description SVector<N>& x The point in \\mathbb{R}^N where to evaluate the field SVector < N > getGradientApprox ( const SVector < N >& x , double step ) const Returns the numerical approximation of the gradient of f at point x. The partial derivatives of the field are approximated via central differences according to the following expression ( e_i denoting the normal unit vector along direction i ) \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x - he_i)}{2h} Args Description const SVector<N>& x The point in \\mathbb{R}^N where to approximate the gradient of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. SMatrix < N > getHessianApprox ( const SVector < N >& x , double step ) const Returns the numerical approximation of the hessian matrix of f at point x. The partial derivatives of the field are approximated via central differences according to the following expressions ( e_i denoting the normal unit vector along direction i ) \\begin{aligned} &\\frac{\\partial^2 f}{\\partial x_i \\partial x_i} &&\\approx \\frac{-f(x + 2he_i) + 16f(x + he_i) - 30f(x) + 16f(x - he_i) - f(x - 2he_i)}{12h^2} \\\\ &\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} &&\\approx \\frac{f(x + he_i + he_j) - f(x + he_i - he_j) - 16f(x - he_i + he_j) + f(x - he_i - he_j)}{4h^2} \\end{aligned} Args Description const SVector<N>& x The point in \\mathbb{R}^N where to approximate the hessian of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems.","title":"Methods"},{"location":"ScalarField/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // create ScalarField ScalarField < 2 > field ( g ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << fun . evaluateAtPoint ( SVector < 2 > ( 4 , 1 )) << std :: endl ; SVector < 2 > grad = fun . getGradientApprox ( SVector < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; SMatrix < 2 > hessian = fun . getHessianApprox ( SVector < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of hessian at point\" << std :: endl ; std :: cout << hessian << std :: endl ;","title":"Examples"},{"location":"TwiceDifferentiableScalarField/","text":"TwiceDifferentiableScalarField core/OPT/ScalarField.h Extends: DifferentiableScalarField Template class used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose hessian function H(f) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times N} is known analitically at every point. template < unsigned int N > class TwiceDifferentiableScalarField : public DifferentiableScalarField < N > { ... }; Methods TwiceDifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ , std :: function < SMatrix < N > ( SVector < N > ) > ddf_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression std::function<SMatrix<N>(SVector<N>)> ddf_ An std::function implementing the analytical expression of the hessian function H(f) taking an N dimensional point in input and returning an N dimensional square matrix in output representing the hessian expression std :: function < SMatrix < N > ( SVector < N > ) > deriveTwice () const override ; Returns the analytical expression of the field's hessian H(f) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times N} as a Callable object. Examples Define a scalar field f(x,y) = 2x^2 + 2y^2 having exact gradient equal to \\nabla f = [4x, 4y]^T and exact hessian matrix given by H(f) = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4 \\end{bmatrix} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // define the field analytical expression: 2*x^2 + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ); }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ], 4 * x [ 1 ]}); }; // define analytical expression of hessian matrix std :: function < SMatrix < 2 > ( SVector < 2 > ) > ddg = []( SVector < 2 > x ) -> SMatrix < 2 > { return SMatrix < 2 > ({{ 4 , 0 }, { 0 , 4 }}); }; // define twice differentiable field TwiceDifferentiableScalarField < 2 > field ( g , dg , ddg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVector < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of hessian at point SVector < 2 > hess = field . getHessianApprox ( SVector < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << hess << std :: endl ; // evaluate exact hessian at point SVector < 2 > exactHess = field . deriveTwice ()( SVector < 2 > ({ 2 , 1 })); std :: cout << \"exact hessian at point\" << std :: endl ; std :: cout << exactHess << std :: endl ;","title":"TwiceDifferentiableScalarField"},{"location":"TwiceDifferentiableScalarField/#twicedifferentiablescalarfield","text":"core/OPT/ScalarField.h Extends: DifferentiableScalarField Template class used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose hessian function H(f) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times N} is known analitically at every point. template < unsigned int N > class TwiceDifferentiableScalarField : public DifferentiableScalarField < N > { ... };","title":"TwiceDifferentiableScalarField"},{"location":"TwiceDifferentiableScalarField/#methods","text":"TwiceDifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ , std :: function < SMatrix < N > ( SVector < N > ) > ddf_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression std::function<SMatrix<N>(SVector<N>)> ddf_ An std::function implementing the analytical expression of the hessian function H(f) taking an N dimensional point in input and returning an N dimensional square matrix in output representing the hessian expression std :: function < SMatrix < N > ( SVector < N > ) > deriveTwice () const override ; Returns the analytical expression of the field's hessian H(f) : \\mathbb{R}^N \\rightarrow \\mathbb{R}^{N \\times N} as a Callable object.","title":"Methods"},{"location":"TwiceDifferentiableScalarField/#examples","text":"Define a scalar field f(x,y) = 2x^2 + 2y^2 having exact gradient equal to \\nabla f = [4x, 4y]^T and exact hessian matrix given by H(f) = \\begin{bmatrix} 4 & 0 \\\\ 0 & 4 \\end{bmatrix} 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // define the field analytical expression: 2*x^2 + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ); }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ], 4 * x [ 1 ]}); }; // define analytical expression of hessian matrix std :: function < SMatrix < 2 > ( SVector < 2 > ) > ddg = []( SVector < 2 > x ) -> SMatrix < 2 > { return SMatrix < 2 > ({{ 4 , 0 }, { 0 , 4 }}); }; // define twice differentiable field TwiceDifferentiableScalarField < 2 > field ( g , dg , ddg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVector < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of hessian at point SVector < 2 > hess = field . getHessianApprox ( SVector < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << hess << std :: endl ; // evaluate exact hessian at point SVector < 2 > exactHess = field . deriveTwice ()( SVector < 2 > ({ 2 , 1 })); std :: cout << \"exact hessian at point\" << std :: endl ; std :: cout << exactHess << std :: endl ;","title":"Examples"},{"location":"doc/","text":"Documentation core optimization Optimizer GridOptimizer NewtonOptimizer NewtonForwardDifferenceOptimizer GradientDescentOptimizer mesh handling FEM approximation cose che metto a caso e poi si vede: concetto di funzione o objective function models regression FPCA density estimation interfaces R python in dubbio validation (CV, K-fold CV, GCV) dipendenze Rcpp for R/C++ communication RcppEigen: Rcpp Integration for the Eigen Templated Linear Algebra Library Eigen","title":"Documentation"},{"location":"doc/#documentation","text":"","title":"Documentation"},{"location":"doc/#core","text":"optimization Optimizer GridOptimizer NewtonOptimizer NewtonForwardDifferenceOptimizer GradientDescentOptimizer mesh handling FEM approximation cose che metto a caso e poi si vede: concetto di funzione o objective function","title":"core"},{"location":"doc/#models","text":"regression FPCA density estimation","title":"models"},{"location":"doc/#interfaces","text":"R python","title":"interfaces"},{"location":"doc/#in-dubbio","text":"validation (CV, K-fold CV, GCV)","title":"in dubbio"},{"location":"doc/#dipendenze","text":"Rcpp for R/C++ communication RcppEigen: Rcpp Integration for the Eigen Templated Linear Algebra Library Eigen","title":"dipendenze"},{"location":"start/","text":"Getting started","title":"Getting started"},{"location":"start/#getting-started","text":"","title":"Getting started"}]}