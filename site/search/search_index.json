{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to fdaPDE functional data analysis with partial differential equation regularization. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Method Description Content Cell Content Cell Content Cell Content Cell Note use a derivede class to iterate on the elements Important important advice We try to minimize the following functional J_\\lambda(\\beta,f) = \\sum_{i=0}^n |z_i - w_i^T \\beta - f(p_i)|^2 - \\lambda \\int_\\Omega(Lf - u)^2","title":"Home"},{"location":"#welcome-to-fdapde","text":"functional data analysis with partial differential equation regularization.","title":"Welcome to fdaPDE"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Method Description Content Cell Content Cell Content Cell Content Cell Note use a derivede class to iterate on the elements Important important advice We try to minimize the following functional J_\\lambda(\\beta,f) = \\sum_{i=0}^n |z_i - w_i^T \\beta - f(p_i)|^2 - \\lambda \\int_\\Omega(Lf - u)^2","title":"Project layout"},{"location":"DifferentiableScalarField/","text":"DifferentiableScalarField Extends ScalarField , Differentiable This template class is used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose gradient function \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N is known analitically at every point. template < unsigned int N > class DifferentiableScalarField : public ScalarField < N > , public Differentiable < N > { ... }; Note Differentiable is just an interface forcing the extending class to define an analytical expression for the gradient of the field. Methods DifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression Examples Define a scalar field f(x,y) = 2x^2 - 2y^2x having exact gradient equal to \\nabla f = [4x - 2y^2, -4yx]^T 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( Point < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ] - 2 * std :: pow ( x [ 1 ], 2 ), -4 * x [ 1 ] * x [ 0 ]}); }; // define differentiable field DifferentiableScalarField < 2 > field ( g , dg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVect < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of gradient at point SVect < 2 > grad = field . getGradientApprox ( SVect < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; // evaluate exact gradient at point SVect < 2 > exactGrad = field . derive ()( SVect < 2 > ({ 2 , 1 })); std :: cout << \"exact gradient at point\" << std :: endl ; std :: cout << exactGrad << std :: endl ;","title":"DifferentiableScalarField"},{"location":"DifferentiableScalarField/#differentiablescalarfield","text":"Extends ScalarField , Differentiable This template class is used to represent a scalar field f : \\mathbb{R}^N \\rightarrow \\mathbb{R} whose gradient function \\nabla f : \\mathbb{R}^N \\rightarrow \\mathbb{R}^N is known analitically at every point. template < unsigned int N > class DifferentiableScalarField : public ScalarField < N > , public Differentiable < N > { ... }; Note Differentiable is just an interface forcing the extending class to define an analytical expression for the gradient of the field.","title":"DifferentiableScalarField"},{"location":"DifferentiableScalarField/#methods","text":"DifferentiableScalarField ( std :: function < double ( SVector < N > ) > f_ , std :: function < SVector < N > ( SVector < N > ) > df_ ) Constructor Args Description std::function<double(SVector<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output std::function<SVector<N>(SVector<N>)> df_ An std::function implementing the analytical expression of the vector field \\nabla f taking an N dimensional point in input and returning an N dimensional point in output representing the gradient expression","title":"Methods"},{"location":"DifferentiableScalarField/#examples","text":"Define a scalar field f(x,y) = 2x^2 - 2y^2x having exact gradient equal to \\nabla f = [4x - 2y^2, -4yx]^T 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( SVector < 2 > ) > g = []( Point < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // define analytical expression of gradient field std :: function < SVector < 2 > ( SVector < 2 > ) > dg = []( SVector < 2 > x ) -> SVector < 2 > { return SVector < 2 > ({ 4 * x [ 0 ] - 2 * std :: pow ( x [ 1 ], 2 ), -4 * x [ 1 ] * x [ 0 ]}); }; // define differentiable field DifferentiableScalarField < 2 > field ( g , dg ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << field . evaluateAtPoint ( SVect < 2 > ({ 4 , 1 })) << std :: endl ; // get approximation of gradient at point SVect < 2 > grad = field . getGradientApprox ( SVect < 2 > ({ 2 , 1 }), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; // evaluate exact gradient at point SVect < 2 > exactGrad = field . derive ()( SVect < 2 > ({ 2 , 1 })); std :: cout << \"exact gradient at point\" << std :: endl ; std :: cout << exactGrad << std :: endl ;","title":"Examples"},{"location":"ExactNewtonOptimizer/","text":"ExactNewtonOptimizer Extends NewtonOptimizer Template class to optimize a given TwiceDifferentiableScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class ExactNewtonOptimizer : public NewtonOptimizer < N > { ... }; Note The implementation of Newton's method provided by this class relies on the exact analytical expression of gradient and hessian of the field. See parent class NewtonOptimizer in case you want to use numerically approximations for these quantities or you have no analytical expressions for them. Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting . Methods ExactNewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimumExact ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , const TwiceDifferentiableScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. TwiceDifferentiableScalarField<N>& objective The objective function to optimize encoded as a TwiceDifferentiableScalarField to have access to exact gradient and hessian expressions. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations. Examples The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"ExactNewtonOptimizer"},{"location":"ExactNewtonOptimizer/#exactnewtonoptimizer","text":"Extends NewtonOptimizer Template class to optimize a given TwiceDifferentiableScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class ExactNewtonOptimizer : public NewtonOptimizer < N > { ... }; Note The implementation of Newton's method provided by this class relies on the exact analytical expression of gradient and hessian of the field. See parent class NewtonOptimizer in case you want to use numerically approximations for these quantities or you have no analytical expressions for them. Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting .","title":"ExactNewtonOptimizer"},{"location":"ExactNewtonOptimizer/#methods","text":"ExactNewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimumExact ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , const TwiceDifferentiableScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. TwiceDifferentiableScalarField<N>& objective The objective function to optimize encoded as a TwiceDifferentiableScalarField to have access to exact gradient and hessian expressions. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations.","title":"Methods"},{"location":"ExactNewtonOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"Examples"},{"location":"Function/","text":"Function","title":"Function"},{"location":"Function/#function","text":"","title":"Function"},{"location":"GridOptimizer/","text":"GridOptimizer Template class to optimize a given function over an N-dimensional grid of equidistant nodes. template < unsigned int N > class GridOptimizer { ... }; The class works by constructing an N-dimensional rectangle [a_1, b_1] \\times \\ldots \\times [a_N, b_N] and splitting each interval according to a given grid step (the grid step can possibly differ for each dimension). The search is performed using an exhaustive search over the N-dimensional grid. Warning Since this optimization algorithm applies just an exhaustive search without any heuristic the resulting time complexity is exponential in the dimension of the grid N. The method can be very slow for values of N bigger than 2. Methods GridOptimizer ( std :: array < std :: pair < double , double > , N > domain , std :: array < double , N > steps ) Constructor initializing the class. It sets the shape of the domain where to search for the optimum of the objective as well as the grid step for each dimension Args Description std::array<std::pair<double,double>,N> An array of exactly N pairs where pair i represents infimum and superior limit of the interval [a_i, b_i] along the i -th dimension std::array<double,N> steps An array of exactly N doubles where the i -th element defines the grid step along the i -th dimension findMinimum ( std :: function < double ( array < double , N > ) > objective ) const Apply the search strategy to the objective passed as argument. Returns std::pair<array<double, N>,double> where the first element is the point in the grid where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description std::function<double(array<double, N>)> objective Any std::function returning a double and taking as input an array of exactly N double representing the objective function f : \\mathbb{R}^N \\rightarrow \\mathbb{R} to optimize Note During the search no grid is actually stored in memory making the call at least memory efficient. Examples The following code finds the minimum of function g(x,y) = 2x^2 - 2y^2 over a grid of points defined in [1,5] \\times [1,5] \\subset \\mathbb{R}^2 with grid step 0.01 for each dimension 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 - 2*y^2 std :: function < double ( std :: array < double , 2 > ) > g = []( std :: array < double , 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ); }; // perform a 2D grid optimization // set optimization domain: [1,5] x [1,5] std :: array < std :: pair < double , double > , 2 > domain2D = { std :: pair < double , double > ( 1 , 5 ), std :: pair < double , double > ( 1 , 5 ) }; // set grid step size std :: array < double , 2 > step2D = { 0.01 , 0.01 }; // create optimizer GridOptimizer < 2 > opt2D ( domain2D , lambda2D ); // find minimum of g std :: pair < array < double , 2 > , double > min_g = opt2D . findMinimum ( g );","title":"GridOptimizer"},{"location":"GridOptimizer/#gridoptimizer","text":"Template class to optimize a given function over an N-dimensional grid of equidistant nodes. template < unsigned int N > class GridOptimizer { ... }; The class works by constructing an N-dimensional rectangle [a_1, b_1] \\times \\ldots \\times [a_N, b_N] and splitting each interval according to a given grid step (the grid step can possibly differ for each dimension). The search is performed using an exhaustive search over the N-dimensional grid. Warning Since this optimization algorithm applies just an exhaustive search without any heuristic the resulting time complexity is exponential in the dimension of the grid N. The method can be very slow for values of N bigger than 2.","title":"GridOptimizer"},{"location":"GridOptimizer/#methods","text":"GridOptimizer ( std :: array < std :: pair < double , double > , N > domain , std :: array < double , N > steps ) Constructor initializing the class. It sets the shape of the domain where to search for the optimum of the objective as well as the grid step for each dimension Args Description std::array<std::pair<double,double>,N> An array of exactly N pairs where pair i represents infimum and superior limit of the interval [a_i, b_i] along the i -th dimension std::array<double,N> steps An array of exactly N doubles where the i -th element defines the grid step along the i -th dimension findMinimum ( std :: function < double ( array < double , N > ) > objective ) const Apply the search strategy to the objective passed as argument. Returns std::pair<array<double, N>,double> where the first element is the point in the grid where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description std::function<double(array<double, N>)> objective Any std::function returning a double and taking as input an array of exactly N double representing the objective function f : \\mathbb{R}^N \\rightarrow \\mathbb{R} to optimize Note During the search no grid is actually stored in memory making the call at least memory efficient.","title":"Methods"},{"location":"GridOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 - 2y^2 over a grid of points defined in [1,5] \\times [1,5] \\subset \\mathbb{R}^2 with grid step 0.01 for each dimension 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 - 2*y^2 std :: function < double ( std :: array < double , 2 > ) > g = []( std :: array < double , 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ); }; // perform a 2D grid optimization // set optimization domain: [1,5] x [1,5] std :: array < std :: pair < double , double > , 2 > domain2D = { std :: pair < double , double > ( 1 , 5 ), std :: pair < double , double > ( 1 , 5 ) }; // set grid step size std :: array < double , 2 > step2D = { 0.01 , 0.01 }; // create optimizer GridOptimizer < 2 > opt2D ( domain2D , lambda2D ); // find minimum of g std :: pair < array < double , 2 > , double > min_g = opt2D . findMinimum ( g );","title":"Examples"},{"location":"NewtonOptimizer/","text":"NewtonOptimizer Template class to optimize a given ScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class NewtonOptimizer { ... }; Note The implementation of Newton's method provided by this class works by numerically approximating both gradient and hessian of the field at the given point. See ExactNewtonOptimizer in case you want to use the analytical expression of these quantites during the iterative process Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting . Methods NewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimum ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , ScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. ScalarField<N>& objective The objective function to optimize. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations. Examples The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"NewtonOptimizer"},{"location":"NewtonOptimizer/#newtonoptimizer","text":"Template class to optimize a given ScalarField over \\mathbb{R}^N using the Newton's iterative optimization method: x_{n+1} = x_{n} - \\lambda H_f(x_n)^{-1} \\cdot \\nabla f(x_n) where H_f(x_n) \\in \\mathbb{R}^{N \\times N} denotes the Hessian matrix of the field evaluated at point x_n . template < unsigned int N > class NewtonOptimizer { ... }; Note The implementation of Newton's method provided by this class works by numerically approximating both gradient and hessian of the field at the given point. See ExactNewtonOptimizer in case you want to use the analytical expression of these quantites during the iterative process Info At each iteration of the method to avoid the cost of matrix inversion, the quantity H_f(x_n)^{-1} \\cdot \\nabla f(x_n) is computed by solving the linear system H_f(x_n) z = \\nabla f(x_n) in z using eigen's QR decompostion with column-pivoting .","title":"NewtonOptimizer"},{"location":"NewtonOptimizer/#methods","text":"NewtonOptimizer(double step_) Constructor initializing some quantities in the internal representation of the class. Args Description double step_ The term \\lambda in the iterative formulation of the Newton's method. std :: pair < SVector < N > , double > findMinimum ( const SVector < N >& x0 , unsigned int maxIteration , double tolerance , ScalarField < N >& objective ) const Applies the optimization method to the objective passed as argument. Returns std::pair<SVector<N>,double> where the first element is the point in \\mathbb{R}^N where the minimum is reached while the second one is the actual minimum value reached by the objective. Args Description const SVector<N>& x0 The initial point from which the iterative method is started. unsigned int maxIteration The maximum number of iterations allowed. double tolerance The tolerance on the error of the obtained solution requested from the method. ScalarField<N>& objective The objective function to optimize. Note The method stops either if the l^2 norm of the gradient of the objective \\left\\lVert \\nabla f(x_n) \\right\\rVert reaches the required tolerance or if such tolerance is not reached before a maxIteration number of iterations.","title":"Methods"},{"location":"NewtonOptimizer/#examples","text":"The following code finds the minimum of function g(x,y) = 2x^2 + x + 2y^2 using Newton's method with \\lambda = 0.01 starting from point (1,1) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // define target to optimize: 2*x^2 + x + 2*y^2 std :: function < double ( SVector < 2 > ) > g = []( SVector < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) + 2 * std :: pow ( x [ 1 ], 2 ) + x [ 0 ]; }; // wrap target in a ScalarField object ScalarField < 2 > objective ( g ); // perform newton optimization // set learning rate double lambda = 0.01 ; // create optimizer NewtonOptimizer < 2 > optNewton2D ( lambda ); // find minimum of g unsigned int max_iterations = 1000 ; double tolerance = 0.001 ; std :: pair < SVector < 2 > , double > min_g = optNewton2D . findMinimum ( SVector < 2 > ( 1 , 1 ), max_iterations , tolerance , objective );","title":"Examples"},{"location":"R/","text":"RfdaPDE This class is the main entry point to the core C++ library from R. Note fdaPDE uses Rcpp and RcppEigen to interface the core library to R. For more informations see Rcpp: Seamless R and C++ Integration RcppEigen: R and Eigen via Rcpp The fdaPDE interface can be loaded from R using the following instructions library(\"Rcpp\") Rcpp::sourceCpp(\"RfdaPDE.cpp\") # create an fdaPDE interface accessible from R fdaPDE_interface = new(RfdaPDE) # you can now access methods using the $ notation fdaPDE_interface$qualcosa ... Interface extension To provide new functionalities to the outside of the C++ core library is required to add a method to the RfdaPDE class working as entry point.","title":"R Interface"},{"location":"R/#rfdapde","text":"This class is the main entry point to the core C++ library from R. Note fdaPDE uses Rcpp and RcppEigen to interface the core library to R. For more informations see Rcpp: Seamless R and C++ Integration RcppEigen: R and Eigen via Rcpp The fdaPDE interface can be loaded from R using the following instructions library(\"Rcpp\") Rcpp::sourceCpp(\"RfdaPDE.cpp\") # create an fdaPDE interface accessible from R fdaPDE_interface = new(RfdaPDE) # you can now access methods using the $ notation fdaPDE_interface$qualcosa ...","title":"RfdaPDE"},{"location":"R/#interface-extension","text":"To provide new functionalities to the outside of the C++ core library is required to add a method to the RfdaPDE class working as entry point.","title":"Interface extension"},{"location":"ScalarField/","text":"ScalarField A template class for handling scalar fields f : \\mathbb{R}^N \\rightarrow \\mathbb{R} template < unsigned int N > class ScalarField { ... }; Note A field wrapped by this template doesn't guarantee any regularity condition. You can wrap any scalar field which is just evaluable at any point. The definition of a formal derivative is not required. See DifferentiableScalarField or TwiceDifferentiableScalarField for inherited classes which assume specific regularity conditions. Methods ScalarField ( std :: function < double ( Point < N > ) > f_ ) Just sets the internal representation of the field to the functional f_ passed as argument. Args Description std::function<double(Point<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output inline double evaluateAtPoint ( Point < N >& x ) Returns the evaluation of the field at point x. Args Description Point<N>& x The point in \\mathbb{R}^N where to evaluate the field Point < N > getGradientApprox ( Point < N >& x , double step ) const Returns the numerical approximation of the gradient of f at point x. The partial derivatives of the field are approximated via central differences according to the following expression ( e_i denoting the normal unit vector along direction i ) \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x - he_i)}{2h} Args Description Point<N>& x The point in \\mathbb{R}^N where to approximate the gradient of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. Eigen :: Matrix < double , N , N > getHessianApprox ( Point < N >& x , double step ) const Returns the numerical approximation of the hessian matrix of f at point x. The partial derivatives of the field are approximated via central differences according to the following expressions ( e_i denoting the normal unit vector along direction i ) \\begin{aligned} &\\frac{\\partial^2 f}{\\partial x_i \\partial x_i} &&\\approx \\frac{-f(x + 2he_i) + 16f(x + he_i) - 30f(x) + 16f(x - he_i) - f(x - 2he_i)}{12h^2} \\\\ &\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} &&\\approx \\frac{f(x + he_i + he_j) - f(x + he_i - he_j) - 16f(x - he_i + he_j) + f(x - he_i - he_j)}{4h^2} \\end{aligned} Args Description Point<N>& x The point in \\mathbb{R}^N where to approximate the hessian of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. Examples 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( Point < 2 > ) > g = []( Point < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // create ScalarField ScalarField < 2 > field ( g ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << fun . evaluateAtPoint ( Point < 2 > ( 4 , 1 )) << std :: endl ; Point < 2 > grad = fun . getGradientApprox ( Point < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; Eigen :: Matrix < double , 2 , 2 > hessian = fun . getHessianApprox ( Point < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of hessian at point\" << std :: endl ; std :: cout << hessian << std :: endl ;","title":"ScalarField"},{"location":"ScalarField/#scalarfield","text":"A template class for handling scalar fields f : \\mathbb{R}^N \\rightarrow \\mathbb{R} template < unsigned int N > class ScalarField { ... }; Note A field wrapped by this template doesn't guarantee any regularity condition. You can wrap any scalar field which is just evaluable at any point. The definition of a formal derivative is not required. See DifferentiableScalarField or TwiceDifferentiableScalarField for inherited classes which assume specific regularity conditions.","title":"ScalarField"},{"location":"ScalarField/#methods","text":"ScalarField ( std :: function < double ( Point < N > ) > f_ ) Just sets the internal representation of the field to the functional f_ passed as argument. Args Description std::function<double(Point<N>)> f_ An std::function implementing the analytical expression of the field f taking an N dimensional point in input and returning a double in output inline double evaluateAtPoint ( Point < N >& x ) Returns the evaluation of the field at point x. Args Description Point<N>& x The point in \\mathbb{R}^N where to evaluate the field Point < N > getGradientApprox ( Point < N >& x , double step ) const Returns the numerical approximation of the gradient of f at point x. The partial derivatives of the field are approximated via central differences according to the following expression ( e_i denoting the normal unit vector along direction i ) \\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x + he_i) - f(x - he_i)}{2h} Args Description Point<N>& x The point in \\mathbb{R}^N where to approximate the gradient of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems. Eigen :: Matrix < double , N , N > getHessianApprox ( Point < N >& x , double step ) const Returns the numerical approximation of the hessian matrix of f at point x. The partial derivatives of the field are approximated via central differences according to the following expressions ( e_i denoting the normal unit vector along direction i ) \\begin{aligned} &\\frac{\\partial^2 f}{\\partial x_i \\partial x_i} &&\\approx \\frac{-f(x + 2he_i) + 16f(x + he_i) - 30f(x) + 16f(x - he_i) - f(x - 2he_i)}{12h^2} \\\\ &\\frac{\\partial^2 f}{\\partial x_i \\partial x_j} &&\\approx \\frac{f(x + he_i + he_j) - f(x + he_i - he_j) - 16f(x - he_i + he_j) + f(x - he_i - he_j)}{4h^2} \\end{aligned} Args Description Point<N>& x The point in \\mathbb{R}^N where to approximate the hessian of the field double step The value of h in the central difference formula Warning In principle the approximation of the partial derivatives require the field f to be just evaluable at point x. Anyway if the field is not differentiable at point x the approximation might cause problems.","title":"Methods"},{"location":"ScalarField/#examples","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // define the field analytical expression: 2*x^2 - 2*y^2*x std :: function < double ( Point < 2 > ) > g = []( Point < 2 > x ) -> double { return 2 * std :: pow ( x [ 0 ], 2 ) - 2 * std :: pow ( x [ 1 ], 2 ) * x [ 0 ]; }; // create ScalarField ScalarField < 2 > field ( g ); std :: cout << \"evaluation of field at point\" << std :: endl ; std :: cout << fun . evaluateAtPoint ( Point < 2 > ( 4 , 1 )) << std :: endl ; Point < 2 > grad = fun . getGradientApprox ( Point < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of gradient at point\" << std :: endl ; std :: cout << grad << std :: endl ; Eigen :: Matrix < double , 2 , 2 > hessian = fun . getHessianApprox ( Point < 2 > ( 2 , 1 ), 0.001 ); std :: cout << \"approximation of hessian at point\" << std :: endl ; std :: cout << hessian << std :: endl ;","title":"Examples"},{"location":"TwiceDifferentiableScalarField/","text":"TwiceDifferentiableScalarField Extends ScalarField , TwiceDifferentiable","title":"TwiceDifferentiableScalarField"},{"location":"TwiceDifferentiableScalarField/#twicedifferentiablescalarfield","text":"Extends ScalarField , TwiceDifferentiable","title":"TwiceDifferentiableScalarField"},{"location":"doc/","text":"Documentation core optimization Optimizer GridOptimizer NewtonOptimizer NewtonForwardDifferenceOptimizer GradientDescentOptimizer mesh handling FEM approximation cose che metto a caso e poi si vede: concetto di funzione o objective function models regression FPCA density estimation interfaces R python in dubbio validation (CV, K-fold CV, GCV) dipendenze Rcpp for R/C++ communication RcppEigen: Rcpp Integration for the Eigen Templated Linear Algebra Library Eigen","title":"Documentation"},{"location":"doc/#documentation","text":"","title":"Documentation"},{"location":"doc/#core","text":"optimization Optimizer GridOptimizer NewtonOptimizer NewtonForwardDifferenceOptimizer GradientDescentOptimizer mesh handling FEM approximation cose che metto a caso e poi si vede: concetto di funzione o objective function","title":"core"},{"location":"doc/#models","text":"regression FPCA density estimation","title":"models"},{"location":"doc/#interfaces","text":"R python","title":"interfaces"},{"location":"doc/#in-dubbio","text":"validation (CV, K-fold CV, GCV)","title":"in dubbio"},{"location":"doc/#dipendenze","text":"Rcpp for R/C++ communication RcppEigen: Rcpp Integration for the Eigen Templated Linear Algebra Library Eigen","title":"dipendenze"},{"location":"start/","text":"Getting started","title":"Getting started"},{"location":"start/#getting-started","text":"","title":"Getting started"}]}